{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "english_to_python1_working_python_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2b65b35f76874278bcf827215f1615b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fe8754ffb1494d69a8825b54550ed9e0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_acc39be2e31b42569fc1e12e8ed01fdf",
              "IPY_MODEL_55a83940408d496aa63a700ca44e35aa"
            ]
          }
        },
        "fe8754ffb1494d69a8825b54550ed9e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "acc39be2e31b42569fc1e12e8ed01fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6899763a5f764b83911ac6bf3e12b36f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22e434bfe6d34243a0dd04f58212758b"
          }
        },
        "55a83940408d496aa63a700ca44e35aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3913da1738b741c38eea6dcd58767577",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/? [00:04&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a52eff71f335429dbfcffaec17b898d3"
          }
        },
        "6899763a5f764b83911ac6bf3e12b36f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22e434bfe6d34243a0dd04f58212758b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3913da1738b741c38eea6dcd58767577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a52eff71f335429dbfcffaec17b898d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d07b3ccc6d6c47a29a3ea06add4365a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_64880fa2bd2541278539d8ddd71e8d8d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_144e43122da747fcbc4179c0084d466b",
              "IPY_MODEL_736bd852f30248ddb4ba09f01861934f"
            ]
          }
        },
        "64880fa2bd2541278539d8ddd71e8d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "144e43122da747fcbc4179c0084d466b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a94d83ebd9b64b1495424126024f85ba",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4727,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4727,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e9cb270bc99b4668bc85b6746b042ada"
          }
        },
        "736bd852f30248ddb4ba09f01861934f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_acd21c7dbcbb46c686069b00aca4d8ee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4727/4727 [00:00&lt;00:00, 5960.52it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_37f4a53b49e94bf3b4a5536c1ada3645"
          }
        },
        "a94d83ebd9b64b1495424126024f85ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e9cb270bc99b4668bc85b6746b042ada": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "acd21c7dbcbb46c686069b00aca4d8ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "37f4a53b49e94bf3b4a5536c1ada3645": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3bb193b50db418d9c957245b46f6c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8a04de5b7c954b26a5f1add514f6801a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bbf4e18f9e534abc8f5bbe9bc1f20b30",
              "IPY_MODEL_66a308f33fb746e192daa593127de68e"
            ]
          }
        },
        "8a04de5b7c954b26a5f1add514f6801a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbf4e18f9e534abc8f5bbe9bc1f20b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b07b6ff7f6994873bcfc40f5838ec2ac",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4727,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4727,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_551ba03542414e5a88086c3c8c5e0734"
          }
        },
        "66a308f33fb746e192daa593127de68e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_de0c250e5e254789a111850ddbfe6442",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4727/4727 [00:03&lt;00:00, 1228.23it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce6e71c0fdc4409cb7360354fbdbd70c"
          }
        },
        "b07b6ff7f6994873bcfc40f5838ec2ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "551ba03542414e5a88086c3c8c5e0734": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de0c250e5e254789a111850ddbfe6442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce6e71c0fdc4409cb7360354fbdbd70c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c15qqV0Rs4m",
        "outputId": "33f210d7-3634-477c-e2f8-bc19a609c666"
      },
      "source": [
        "# Mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_ZHOF0NR10n"
      },
      "source": [
        "! cp '/content/drive/My Drive/END/session-15/dataset_python_cleaned_final.txt' .\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUnMLdevEzFT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzErHAsEzFU"
      },
      "source": [
        "Then set a random seed for deterministic results/reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_wP4J4LEzFX"
      },
      "source": [
        "SEED = 1238\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hawianoFEzFY"
      },
      "source": [
        "Instantiate English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BP3YSvJEzFY",
        "outputId": "5420be30-55e9-4fe4-b438-bb5ea5f2a15e"
      },
      "source": [
        "%%bash\n",
        "python -m spacy download en\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.1.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2mâœ” Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAq6OVJKfXm3"
      },
      "source": [
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bEkyPt5EzFY"
      },
      "source": [
        "Tokenize method is defined for English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KaGEZ45EzFZ"
      },
      "source": [
        "import tokenize\n",
        "import io\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "def tokenize_python(text):\n",
        "    \"\"\"\n",
        "    Tokenizes Python Code to list of strings\n",
        "    \"\"\"\n",
        "    python_token_list = []\n",
        "    raised_exception = False\n",
        "    try:\n",
        "        tokens = tokenize.tokenize(io.BytesIO(text.encode('utf-8')).readline)\n",
        "        for five_tuple in tokens:\n",
        "            if five_tuple.type == tokenize.COMMENT:\n",
        "                continue\n",
        "            elif five_tuple.type == tokenize.ENCODING:\n",
        "                continue\n",
        "            elif five_tuple.type == tokenize.INDENT:\n",
        "                python_token_list.append(\"INDENT\")\n",
        "            elif five_tuple.type == tokenize.DEDENT:\n",
        "                python_token_list.append(\"DEDENT\")\n",
        "            elif five_tuple.type == tokenize.NL or five_tuple.type == tokenize.NEWLINE:\n",
        "                python_token_list.append(\"NEWLINE\")\n",
        "            elif five_tuple.type == tokenize.ENDMARKER :\n",
        "                continue\n",
        "            else:\n",
        "                python_token_list.append(five_tuple.string)\n",
        "    except Exception:\n",
        "        raised_exception = True\n",
        "        #print( \"Exception: \", Exception, \" program: \", text)\n",
        "    return python_token_list"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mVT1SFp0aAy"
      },
      "source": [
        "## Download and unzip Cornel Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V9qZho8UnAT"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWx8B8cN8vPN",
        "outputId": "6b6a6f66-501f-4839-d7a8-f84d8cc2cefe"
      },
      "source": [
        "device"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn4KKwW9ov_a",
        "outputId": "92778d47-3c69-431a-bf1d-2fc69825e9c3"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import random\n",
        "import random\n",
        "#import google_trans_new\n",
        "#from google_trans_new import google_translator\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "lem = WordNetLemmatizer()\n",
        "#translator = google_translator()\n",
        "\n",
        "def clean_text(text):\n",
        "    ## lower case\n",
        "    if not isinstance(text, str):\n",
        "      return str(text) \n",
        "    cleaned = text.lower()\n",
        "\n",
        "    urls_pattern = re.compile(r'https?://\\S+|www.\\S+')\n",
        "    cleaned = urls_pattern.sub(r'',cleaned)\n",
        "    \n",
        "    ## remove punctuations\n",
        "    punctuations = string.punctuation\n",
        "    cleaned_temp = \"\".join(character for character in cleaned if character not in punctuations)\n",
        "    \n",
        "    ## remove stopwords \n",
        "    words = cleaned_temp.split()\n",
        "    #stopword_lists = stopwords.words(\"english\")\n",
        "    #cleaned = [word for word in words if word not in stopword_lists]\n",
        "    cleaned = words\n",
        "    \n",
        "    ## normalization - lemmatization\n",
        "    #cleaned = [lem.lemmatize(word, \"v\") for word in cleaned]\n",
        "    #cleaned = [lem.lemmatize(word, \"n\") for word in cleaned]\n",
        "    \n",
        "    ## join \n",
        "    cleaned = \" \".join(cleaned)\n",
        "    return cleaned\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBxOkCU9pZ1a"
      },
      "source": [
        "english_text_python_program_pair_list = []\n",
        "process_python_code=False\n",
        "i=1\n",
        "with open('dataset_python_cleaned_final.txt', 'r', encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        #print(i)\n",
        "        i += 1\n",
        "        if process_python_code==False:\n",
        "            if line.strip() == '':\n",
        "                continue\n",
        "            if line.startswith('#'):\n",
        "                english_text = line\n",
        "                #english_text_list.append(line)\n",
        "                process_python_code=True\n",
        "                python_program=''\n",
        "            else:\n",
        "                print(i, \": \", line)            \n",
        "        else:\n",
        "            if line.strip() == '':\n",
        "                process_python_code=False\n",
        "                english_text_python_program_pair_list.append((english_text, python_program))\n",
        "                python_program=''\n",
        "                english_text =''\n",
        "            if line.lstrip().startswith('#'):\n",
        "                continue\n",
        "            else:\n",
        "                python_program += line"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKvMPHBtrFf_",
        "outputId": "b3595cd0-c997-40fd-ca4d-4870cdc349ed"
      },
      "source": [
        "len(english_text_python_program_pair_list)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4727"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6938tdkLq45F"
      },
      "source": [
        "english_text_list,python_program_list  = zip(*english_text_python_program_pair_list)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkK8ux6srNTr"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'English': english_text_list, 'Python':python_program_list })"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "SKeRBdRwrU8Y",
        "outputId": "b93b7d15-eebb-4a9d-f0a8-cd8c05b43e17"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Python</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td># write a python program to add two numbers \\n</td>\n",
              "      <td>num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td># write a python function to add two user prov...</td>\n",
              "      <td>def add_two_numbers(num1, num2):\\n    sum = nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td># write a program to find and print the larges...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &gt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td># write a program to find and print the smalle...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &lt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td># Write a python function to merge two given l...</td>\n",
              "      <td>def merge_lists(l1, l2):\\n    return l1 + l2\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td># Write a program to check whether a number is...</td>\n",
              "      <td>num = 337\\nif num &gt; 1:\\n   for i in range(2, n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td># Write a python function that prints the fact...</td>\n",
              "      <td>def print_factors(x):\\n   print(f\"The factors ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td># Write a program to find the factorial of a n...</td>\n",
              "      <td>num = 13\\nfactorial = 1\\nif num &lt; 0:\\n   print...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td># Write a python function to print whether a n...</td>\n",
              "      <td>def check_pnz(num):\\n    if num &gt; 0:\\n       p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td># Write a program to print the multiplication ...</td>\n",
              "      <td>num = 9\\nfor i in range(1, 11):\\n   print(f\"{n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             English                                             Python\n",
              "0     # write a python program to add two numbers \\n  num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...\n",
              "1  # write a python function to add two user prov...  def add_two_numbers(num1, num2):\\n    sum = nu...\n",
              "2  # write a program to find and print the larges...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 >= n...\n",
              "3  # write a program to find and print the smalle...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 <= n...\n",
              "4  # Write a python function to merge two given l...     def merge_lists(l1, l2):\\n    return l1 + l2\\n\n",
              "5  # Write a program to check whether a number is...  num = 337\\nif num > 1:\\n   for i in range(2, n...\n",
              "6  # Write a python function that prints the fact...  def print_factors(x):\\n   print(f\"The factors ...\n",
              "7  # Write a program to find the factorial of a n...  num = 13\\nfactorial = 1\\nif num < 0:\\n   print...\n",
              "8  # Write a python function to print whether a n...  def check_pnz(num):\\n    if num > 0:\\n       p...\n",
              "9  # Write a program to print the multiplication ...  num = 9\\nfor i in range(1, 11):\\n   print(f\"{n..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "2b65b35f76874278bcf827215f1615b2",
            "fe8754ffb1494d69a8825b54550ed9e0",
            "acc39be2e31b42569fc1e12e8ed01fdf",
            "55a83940408d496aa63a700ca44e35aa",
            "6899763a5f764b83911ac6bf3e12b36f",
            "22e434bfe6d34243a0dd04f58212758b",
            "3913da1738b741c38eea6dcd58767577",
            "a52eff71f335429dbfcffaec17b898d3",
            "d07b3ccc6d6c47a29a3ea06add4365a6",
            "64880fa2bd2541278539d8ddd71e8d8d",
            "144e43122da747fcbc4179c0084d466b",
            "736bd852f30248ddb4ba09f01861934f",
            "a94d83ebd9b64b1495424126024f85ba",
            "e9cb270bc99b4668bc85b6746b042ada",
            "acd21c7dbcbb46c686069b00aca4d8ee",
            "37f4a53b49e94bf3b4a5536c1ada3645"
          ]
        },
        "id": "ZbxiaS36rfFQ",
        "outputId": "6241cd3e-e957-4960-f23b-7be970df8f9c"
      },
      "source": [
        "from tqdm import tqdm_notebook as tqdm\n",
        "tqdm().pandas() \n",
        "\n",
        "df['English'] = df['English'].progress_apply(lambda txt: clean_text(txt))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b65b35f76874278bcf827215f1615b2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d07b3ccc6d6c47a29a3ea06add4365a6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4727.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "d3bb193b50db418d9c957245b46f6c83",
            "8a04de5b7c954b26a5f1add514f6801a",
            "bbf4e18f9e534abc8f5bbe9bc1f20b30",
            "66a308f33fb746e192daa593127de68e",
            "b07b6ff7f6994873bcfc40f5838ec2ac",
            "551ba03542414e5a88086c3c8c5e0734",
            "de0c250e5e254789a111850ddbfe6442",
            "ce6e71c0fdc4409cb7360354fbdbd70c"
          ]
        },
        "id": "V_i2eO9Ys7PF",
        "outputId": "1d451581-4ae2-4dcc-fbe1-e171c61d6469"
      },
      "source": [
        "df['Python'] = df['Python'].progress_apply(lambda txt: txt.lstrip())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3bb193b50db418d9c957245b46f6c83",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4727.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "0tFqOQ3ZtWLS",
        "outputId": "7d50cd74-fc4d-4047-f47f-f70178906be8"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Python</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>write a python program to add two numbers</td>\n",
              "      <td>num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>write a python function to add two user provid...</td>\n",
              "      <td>def add_two_numbers(num1, num2):\\n    sum = nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>write a program to find and print the largest ...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &gt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>write a program to find and print the smallest...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &lt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>write a python function to merge two given lis...</td>\n",
              "      <td>def merge_lists(l1, l2):\\n    return l1 + l2\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>write a program to check whether a number is p...</td>\n",
              "      <td>num = 337\\nif num &gt; 1:\\n   for i in range(2, n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>write a python function that prints the factor...</td>\n",
              "      <td>def print_factors(x):\\n   print(f\"The factors ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>write a program to find the factorial of a number</td>\n",
              "      <td>num = 13\\nfactorial = 1\\nif num &lt; 0:\\n   print...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>write a python function to print whether a num...</td>\n",
              "      <td>def check_pnz(num):\\n    if num &gt; 0:\\n       p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>write a program to print the multiplication ta...</td>\n",
              "      <td>num = 9\\nfor i in range(1, 11):\\n   print(f\"{n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             English                                             Python\n",
              "0          write a python program to add two numbers  num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...\n",
              "1  write a python function to add two user provid...  def add_two_numbers(num1, num2):\\n    sum = nu...\n",
              "2  write a program to find and print the largest ...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 >= n...\n",
              "3  write a program to find and print the smallest...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 <= n...\n",
              "4  write a python function to merge two given lis...     def merge_lists(l1, l2):\\n    return l1 + l2\\n\n",
              "5  write a program to check whether a number is p...  num = 337\\nif num > 1:\\n   for i in range(2, n...\n",
              "6  write a python function that prints the factor...  def print_factors(x):\\n   print(f\"The factors ...\n",
              "7  write a program to find the factorial of a number  num = 13\\nfactorial = 1\\nif num < 0:\\n   print...\n",
              "8  write a python function to print whether a num...  def check_pnz(num):\\n    if num > 0:\\n       p...\n",
              "9  write a program to print the multiplication ta...  num = 9\\nfor i in range(1, 11):\\n   print(f\"{n..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxa2u-NbYLea"
      },
      "source": [
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext import data"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFrB-gigY70K"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator, Example, Dataset\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsuS29qEYqzU"
      },
      "source": [
        "SRC = Field(tokenize=tokenize_en, \n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>',            \n",
        "            batch_first = True, \n",
        "            lower=True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_python, \n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>', \n",
        "            batch_first = True\n",
        "            )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbA4nkUnY_f7"
      },
      "source": [
        "fields = [('English', SRC),('Python',TRG)]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ1D69mYdpB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61147cd6-3688-46ff-c8ce-bd323d205a94"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, valid = train_test_split(df, test_size=0.02)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBIQ_97Vxgsh"
      },
      "source": [
        "train = train.reset_index(drop=True)\n",
        "valid = valid.reset_index(drop=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxKHMVoiKa4x"
      },
      "source": [
        "MAX_OUTPUT_SEQ_LENGTH = 100"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbqQ8OZCeg_R"
      },
      "source": [
        "example_trng = [Example.fromlist([train.English[i],train.Python[i]], fields) for i in range(train.shape[0]) if len(tokenize_python(train.Python[i])) <= MAX_OUTPUT_SEQ_LENGTH - 4 ] \n",
        "example_val = [Example.fromlist([valid.English[i],valid.Python[i]], fields) for i in range(valid.shape[0]) if len(tokenize_python(valid.Python[i])) <= MAX_OUTPUT_SEQ_LENGTH - 4 ] "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_-_R27Vfcf2"
      },
      "source": [
        "train_dataset = Dataset(example_trng, fields)\n",
        "valid_dataset = Dataset(example_val, fields)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urgbsj1pfrIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9812b568-9107-4d5c-803a-9db042f0ac9c"
      },
      "source": [
        "vars(train_dataset.examples[10])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'English': ['write',\n",
              "  'a',\n",
              "  'python',\n",
              "  'program',\n",
              "  'to',\n",
              "  'print',\n",
              "  'negative',\n",
              "  'numbers',\n",
              "  'in',\n",
              "  'a',\n",
              "  'list'],\n",
              " 'Python': ['list1',\n",
              "  '=',\n",
              "  '[',\n",
              "  '11',\n",
              "  ',',\n",
              "  '-',\n",
              "  '21',\n",
              "  ',',\n",
              "  '0',\n",
              "  ',',\n",
              "  '45',\n",
              "  ',',\n",
              "  '66',\n",
              "  ',',\n",
              "  '-',\n",
              "  '93',\n",
              "  ']',\n",
              "  'NEWLINE',\n",
              "  'for',\n",
              "  'num',\n",
              "  'in',\n",
              "  'list1',\n",
              "  ':',\n",
              "  'NEWLINE',\n",
              "  'INDENT',\n",
              "  'if',\n",
              "  'num',\n",
              "  '<',\n",
              "  '0',\n",
              "  ':',\n",
              "  'NEWLINE',\n",
              "  'INDENT',\n",
              "  'print',\n",
              "  '(',\n",
              "  'num',\n",
              "  ',',\n",
              "  'end',\n",
              "  '=',\n",
              "  '\" \"',\n",
              "  ')',\n",
              "  'NEWLINE',\n",
              "  'DEDENT',\n",
              "  'DEDENT']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKJnJ1c2yunA",
        "outputId": "054dae56-b8e6-464d-d18b-5078a8c969d0"
      },
      "source": [
        "vars(valid_dataset.examples[5])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'English': ['write',\n",
              "  'a',\n",
              "  'python',\n",
              "  'function',\n",
              "  'to',\n",
              "  'reverse',\n",
              "  'words',\n",
              "  'in',\n",
              "  'a',\n",
              "  'string'],\n",
              " 'Python': ['def',\n",
              "  'reverse_string_words',\n",
              "  '(',\n",
              "  'text',\n",
              "  ')',\n",
              "  ':',\n",
              "  'NEWLINE',\n",
              "  'INDENT',\n",
              "  'for',\n",
              "  'line',\n",
              "  'in',\n",
              "  'text',\n",
              "  '.',\n",
              "  'split',\n",
              "  '(',\n",
              "  \"'\\\\n'\",\n",
              "  ')',\n",
              "  ':',\n",
              "  'NEWLINE',\n",
              "  'INDENT',\n",
              "  'return',\n",
              "  '(',\n",
              "  \"' '\",\n",
              "  '.',\n",
              "  'join',\n",
              "  '(',\n",
              "  'line',\n",
              "  '.',\n",
              "  'split',\n",
              "  '(',\n",
              "  ')',\n",
              "  '[',\n",
              "  ':',\n",
              "  ':',\n",
              "  '-',\n",
              "  '1',\n",
              "  ']',\n",
              "  ')',\n",
              "  ')',\n",
              "  'NEWLINE',\n",
              "  'DEDENT',\n",
              "  'DEDENT']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9dIH0Gai-sy"
      },
      "source": [
        "SRC.build_vocab(train_dataset)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZBc8gLAzJha"
      },
      "source": [
        "TRG.build_vocab(train_dataset)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv-YIl7KjnxM"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYOaLBK9jo6Q"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\n",
        "    (train_dataset, valid_dataset), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key=lambda x:len(x.English),\n",
        "    sort_within_batch = False, \n",
        "    device = device)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7rbQLYHEzFZ"
      },
      "source": [
        "Create our fields to process our data. This will append the \"start of sentence\" and \"end of sentence\" tokens as well as converting all words to lowercase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsycLH0DEzFa"
      },
      "source": [
        "Load our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fen1DHy6EzFb"
      },
      "source": [
        "We'll also print out an example just to double check they're not reversed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRDnbA09EzFc"
      },
      "source": [
        "Then create our vocabulary, converting all tokens appearing less than twice into `<unk>` tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2hiioCLEzFd"
      },
      "source": [
        "Finally, define the `device` and create our iterators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq7BwOO4EzFd"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "### Encoder\n",
        "\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer-encoder.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11CnDhTkEzFd"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        #src = (self.tok_embedding(src) * self.scale) + self.pos_embedding(pos)\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykp4dmnLrA3Y"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiKjBoHqrNMz"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBp6e8nlrtCg"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhBViJ4-EzFe"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer-decoder.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRA8hkiLEzFh"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = MAX_OUTPUT_SEQ_LENGTH):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        #print(\"Decoder __init__ : output_dim=\", output_dim, \" max_length=\", max_length, \" hid dim=\", hid_dim)\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        self.max_length = max_length\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        #print(\"Decoder.forward: batch_size: \", batch_size, \"trg_len=\", trg_len )\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        #print(\"Decoder.forward: pos =\", pos)\n",
        "        #print(\"Decoder.forward: trg =\", trg)                    \n",
        "        #pos = [batch size, trg len]\n",
        "        tok_embed = self.tok_embedding(trg)\n",
        "       \n",
        "        #print(\"Decoder.forward: tok_embed =\", tok_embed.shape)\n",
        "        pos_embed = self.pos_embedding(pos)\n",
        "        #print(\"Decoder.forward: pos_embed shape =\", pos_embed.shape)\n",
        "            \n",
        "        #trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "        trg = self.dropout((tok_embed*self.scale) + pos_embed)\n",
        "        #print(\"Decoder.forward: trg=\", trg)\n",
        "        #trg = (self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos)\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #print(\"Decoder.forward: before calling layers\")\n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        #print(\"Decoder.forward: after calling layers: trg=\", trg)\n",
        "        #print(\"Decoder.forward: after calling layers: attention=\", trg)\n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        #print(\"Decoder.forward: before calling fc_out\")\n",
        "        output = self.fc_out(trg)\n",
        "        #print(\"Decoder.forward: after calling fc_out: output=\", output)\n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-QPOMrRr89N"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        # query, key, value\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV5zN4gEsfHX"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "  \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]    \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqR66FM2EzFj"
      },
      "source": [
        "## Seq2Seq Model\n",
        "\n",
        "Putting the encoder and decoder together, we get:\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer1.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxVMMPUyEzFk"
      },
      "source": [
        "# Training the Seq2Seq Model\n",
        "\n",
        "The rest of this session is very similar to the previous one. \n",
        "\n",
        "We initialise our encoder, decoder and seq2seq model (placing it on the GPU if we have one). As before, the embedding dimensions and the amount of dropout used can be different between the encoder and the decoder, but the hidden dimensions must remain the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDG6jOSuEzFk"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPHUOmUH9Alz",
        "outputId": "54a6dcbf-b9a9-409a-88a8-23eb5be67fa7"
      },
      "source": [
        "OUTPUT_DIM"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4816"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToMo3UfHtbHl"
      },
      "source": [
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "SRC_UNK_IDX = SRC.vocab.stoi[SRC.unk_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "TRG_UNK_IDX = TRG.vocab.stoi[TRG.unk_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0dg6wOScJRo",
        "outputId": "2e94517c-6445-4cb8-feb1-fca59251b46c"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(1900, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(4816, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=4816, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7OOAgyOZzQg"
      },
      "source": [
        "enc.tok_embedding.weight.data[SRC_PAD_IDX] = torch.zeros(ENC_EMB_DIM)\n",
        "enc.tok_embedding.weight.data[SRC_UNK_IDX] = torch.zeros(ENC_EMB_DIM)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dElc7zww_UuJ"
      },
      "source": [
        "dec.tok_embedding.weight.data[TRG_PAD_IDX] = torch.zeros(DEC_EMB_DIM)\n",
        "dec.tok_embedding.weight.data[SRC_UNK_IDX] = torch.zeros(DEC_EMB_DIM)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIc1l6CEzFk"
      },
      "source": [
        "Next, we initialize our parameters. The paper states the parameters are initialized from a normal distribution with a mean of 0 and a standard deviation of 0.01, i.e. $\\mathcal{N}(0, 0.01)$. \n",
        "\n",
        "It also states we should initialize the recurrent parameters to a special initialization, however to keep things simple we'll also initialize them to $\\mathcal{N}(0, 0.01)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd1QoOsUEzFl"
      },
      "source": [
        "We print out the number of parameters.\n",
        "\n",
        "Even though we only have a single layer RNN for our encoder and decoder we actually have **more** parameters  than the last model. This is due to the increased size of the inputs to the GRU and the linear layer. However, it is not a significant amount of parameters and causes a minimal amount of increase in training time (~3 seconds per epoch extra)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IggCwIBgEzFl",
        "outputId": "e408c3ab-df40-4cab-d558-385249ba135f"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 6,961,872 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiH54qzHEzFl"
      },
      "source": [
        "We initiaize our optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO1_eoG7EzFl"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwkV3FKlEzFl"
      },
      "source": [
        "We also initialize the loss function, making sure to ignore the loss on `<pad>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0DAbGbcEzFl"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTay7E9rEzFm"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCw7utNlucv5"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.English\n",
        "        trg = batch.Python\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McZu2YzRNKxW"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.English\n",
        "            trg = batch.Python\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rfUx5lhEzFm"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODZTe6_J1mXz"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw022pw0EzFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3042c736-a8a2-4e3e-def0-e3aee055be7e"
      },
      "source": [
        "N_EPOCHS = 100\n",
        "CLIP = 1\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=10, verbose=True)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/END/session-15/english-python.pt')\n",
        "    scheduler.step(train_loss)\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 4s\n",
            "\tTrain Loss: 8.356 | Train PPL: 4255.131\n",
            "\t Val. Loss: 8.128 |  Val. PPL: 3387.947\n",
            "Epoch: 02 | Time: 0m 4s\n",
            "\tTrain Loss: 7.663 | Train PPL: 2128.486\n",
            "\t Val. Loss: 7.066 |  Val. PPL: 1171.066\n",
            "Epoch: 03 | Time: 0m 4s\n",
            "\tTrain Loss: 6.389 | Train PPL: 595.000\n",
            "\t Val. Loss: 5.747 |  Val. PPL: 313.371\n",
            "Epoch: 04 | Time: 0m 4s\n",
            "\tTrain Loss: 5.213 | Train PPL: 183.597\n",
            "\t Val. Loss: 4.838 |  Val. PPL: 126.260\n",
            "Epoch: 05 | Time: 0m 4s\n",
            "\tTrain Loss: 4.619 | Train PPL: 101.394\n",
            "\t Val. Loss: 4.520 |  Val. PPL:  91.837\n",
            "Epoch: 06 | Time: 0m 4s\n",
            "\tTrain Loss: 4.355 | Train PPL:  77.894\n",
            "\t Val. Loss: 4.282 |  Val. PPL:  72.410\n",
            "Epoch: 07 | Time: 0m 4s\n",
            "\tTrain Loss: 4.080 | Train PPL:  59.163\n",
            "\t Val. Loss: 3.991 |  Val. PPL:  54.113\n",
            "Epoch: 08 | Time: 0m 4s\n",
            "\tTrain Loss: 3.764 | Train PPL:  43.112\n",
            "\t Val. Loss: 3.685 |  Val. PPL:  39.829\n",
            "Epoch: 09 | Time: 0m 4s\n",
            "\tTrain Loss: 3.447 | Train PPL:  31.404\n",
            "\t Val. Loss: 3.381 |  Val. PPL:  29.407\n",
            "Epoch: 10 | Time: 0m 4s\n",
            "\tTrain Loss: 3.164 | Train PPL:  23.663\n",
            "\t Val. Loss: 3.123 |  Val. PPL:  22.704\n",
            "Epoch: 11 | Time: 0m 4s\n",
            "\tTrain Loss: 2.933 | Train PPL:  18.778\n",
            "\t Val. Loss: 2.942 |  Val. PPL:  18.953\n",
            "Epoch: 12 | Time: 0m 4s\n",
            "\tTrain Loss: 2.765 | Train PPL:  15.878\n",
            "\t Val. Loss: 2.796 |  Val. PPL:  16.379\n",
            "Epoch: 13 | Time: 0m 4s\n",
            "\tTrain Loss: 2.630 | Train PPL:  13.875\n",
            "\t Val. Loss: 2.698 |  Val. PPL:  14.850\n",
            "Epoch: 14 | Time: 0m 4s\n",
            "\tTrain Loss: 2.523 | Train PPL:  12.464\n",
            "\t Val. Loss: 2.613 |  Val. PPL:  13.639\n",
            "Epoch: 15 | Time: 0m 4s\n",
            "\tTrain Loss: 2.426 | Train PPL:  11.314\n",
            "\t Val. Loss: 2.523 |  Val. PPL:  12.469\n",
            "Epoch: 16 | Time: 0m 4s\n",
            "\tTrain Loss: 2.340 | Train PPL:  10.384\n",
            "\t Val. Loss: 2.454 |  Val. PPL:  11.630\n",
            "Epoch: 17 | Time: 0m 4s\n",
            "\tTrain Loss: 2.269 | Train PPL:   9.673\n",
            "\t Val. Loss: 2.400 |  Val. PPL:  11.027\n",
            "Epoch: 18 | Time: 0m 4s\n",
            "\tTrain Loss: 2.198 | Train PPL:   9.004\n",
            "\t Val. Loss: 2.337 |  Val. PPL:  10.354\n",
            "Epoch: 19 | Time: 0m 4s\n",
            "\tTrain Loss: 2.136 | Train PPL:   8.466\n",
            "\t Val. Loss: 2.286 |  Val. PPL:   9.838\n",
            "Epoch: 20 | Time: 0m 4s\n",
            "\tTrain Loss: 2.080 | Train PPL:   8.008\n",
            "\t Val. Loss: 2.242 |  Val. PPL:   9.414\n",
            "Epoch: 21 | Time: 0m 4s\n",
            "\tTrain Loss: 2.032 | Train PPL:   7.627\n",
            "\t Val. Loss: 2.201 |  Val. PPL:   9.030\n",
            "Epoch: 22 | Time: 0m 4s\n",
            "\tTrain Loss: 1.977 | Train PPL:   7.220\n",
            "\t Val. Loss: 2.160 |  Val. PPL:   8.670\n",
            "Epoch: 23 | Time: 0m 4s\n",
            "\tTrain Loss: 1.926 | Train PPL:   6.863\n",
            "\t Val. Loss: 2.120 |  Val. PPL:   8.328\n",
            "Epoch: 24 | Time: 0m 4s\n",
            "\tTrain Loss: 1.883 | Train PPL:   6.570\n",
            "\t Val. Loss: 2.080 |  Val. PPL:   8.003\n",
            "Epoch: 25 | Time: 0m 4s\n",
            "\tTrain Loss: 1.835 | Train PPL:   6.263\n",
            "\t Val. Loss: 2.043 |  Val. PPL:   7.710\n",
            "Epoch: 26 | Time: 0m 4s\n",
            "\tTrain Loss: 1.791 | Train PPL:   5.993\n",
            "\t Val. Loss: 2.009 |  Val. PPL:   7.459\n",
            "Epoch: 27 | Time: 0m 4s\n",
            "\tTrain Loss: 1.746 | Train PPL:   5.734\n",
            "\t Val. Loss: 1.969 |  Val. PPL:   7.163\n",
            "Epoch: 28 | Time: 0m 4s\n",
            "\tTrain Loss: 1.706 | Train PPL:   5.506\n",
            "\t Val. Loss: 1.940 |  Val. PPL:   6.960\n",
            "Epoch: 29 | Time: 0m 4s\n",
            "\tTrain Loss: 1.667 | Train PPL:   5.296\n",
            "\t Val. Loss: 1.916 |  Val. PPL:   6.794\n",
            "Epoch: 30 | Time: 0m 4s\n",
            "\tTrain Loss: 1.628 | Train PPL:   5.095\n",
            "\t Val. Loss: 1.885 |  Val. PPL:   6.587\n",
            "Epoch: 31 | Time: 0m 4s\n",
            "\tTrain Loss: 1.585 | Train PPL:   4.878\n",
            "\t Val. Loss: 1.833 |  Val. PPL:   6.250\n",
            "Epoch: 32 | Time: 0m 4s\n",
            "\tTrain Loss: 1.546 | Train PPL:   4.692\n",
            "\t Val. Loss: 1.818 |  Val. PPL:   6.160\n",
            "Epoch: 33 | Time: 0m 4s\n",
            "\tTrain Loss: 1.507 | Train PPL:   4.512\n",
            "\t Val. Loss: 1.784 |  Val. PPL:   5.953\n",
            "Epoch: 34 | Time: 0m 4s\n",
            "\tTrain Loss: 1.472 | Train PPL:   4.356\n",
            "\t Val. Loss: 1.751 |  Val. PPL:   5.762\n",
            "Epoch: 35 | Time: 0m 4s\n",
            "\tTrain Loss: 1.436 | Train PPL:   4.205\n",
            "\t Val. Loss: 1.748 |  Val. PPL:   5.741\n",
            "Epoch: 36 | Time: 0m 4s\n",
            "\tTrain Loss: 1.401 | Train PPL:   4.060\n",
            "\t Val. Loss: 1.702 |  Val. PPL:   5.486\n",
            "Epoch: 37 | Time: 0m 4s\n",
            "\tTrain Loss: 1.365 | Train PPL:   3.917\n",
            "\t Val. Loss: 1.675 |  Val. PPL:   5.341\n",
            "Epoch: 38 | Time: 0m 4s\n",
            "\tTrain Loss: 1.331 | Train PPL:   3.784\n",
            "\t Val. Loss: 1.641 |  Val. PPL:   5.162\n",
            "Epoch: 39 | Time: 0m 4s\n",
            "\tTrain Loss: 1.294 | Train PPL:   3.647\n",
            "\t Val. Loss: 1.624 |  Val. PPL:   5.075\n",
            "Epoch: 40 | Time: 0m 4s\n",
            "\tTrain Loss: 1.265 | Train PPL:   3.543\n",
            "\t Val. Loss: 1.601 |  Val. PPL:   4.956\n",
            "Epoch: 41 | Time: 0m 4s\n",
            "\tTrain Loss: 1.232 | Train PPL:   3.428\n",
            "\t Val. Loss: 1.576 |  Val. PPL:   4.835\n",
            "Epoch: 42 | Time: 0m 4s\n",
            "\tTrain Loss: 1.200 | Train PPL:   3.319\n",
            "\t Val. Loss: 1.545 |  Val. PPL:   4.688\n",
            "Epoch: 43 | Time: 0m 4s\n",
            "\tTrain Loss: 1.173 | Train PPL:   3.231\n",
            "\t Val. Loss: 1.526 |  Val. PPL:   4.600\n",
            "Epoch: 44 | Time: 0m 4s\n",
            "\tTrain Loss: 1.141 | Train PPL:   3.131\n",
            "\t Val. Loss: 1.508 |  Val. PPL:   4.519\n",
            "Epoch: 45 | Time: 0m 4s\n",
            "\tTrain Loss: 1.109 | Train PPL:   3.031\n",
            "\t Val. Loss: 1.480 |  Val. PPL:   4.392\n",
            "Epoch: 46 | Time: 0m 4s\n",
            "\tTrain Loss: 1.080 | Train PPL:   2.944\n",
            "\t Val. Loss: 1.454 |  Val. PPL:   4.278\n",
            "Epoch: 47 | Time: 0m 4s\n",
            "\tTrain Loss: 1.056 | Train PPL:   2.876\n",
            "\t Val. Loss: 1.432 |  Val. PPL:   4.188\n",
            "Epoch: 48 | Time: 0m 4s\n",
            "\tTrain Loss: 1.023 | Train PPL:   2.782\n",
            "\t Val. Loss: 1.411 |  Val. PPL:   4.102\n",
            "Epoch: 49 | Time: 0m 4s\n",
            "\tTrain Loss: 0.991 | Train PPL:   2.694\n",
            "\t Val. Loss: 1.394 |  Val. PPL:   4.031\n",
            "Epoch: 50 | Time: 0m 4s\n",
            "\tTrain Loss: 0.957 | Train PPL:   2.605\n",
            "\t Val. Loss: 1.383 |  Val. PPL:   3.987\n",
            "Epoch: 51 | Time: 0m 4s\n",
            "\tTrain Loss: 0.937 | Train PPL:   2.551\n",
            "\t Val. Loss: 1.361 |  Val. PPL:   3.899\n",
            "Epoch: 52 | Time: 0m 4s\n",
            "\tTrain Loss: 0.912 | Train PPL:   2.489\n",
            "\t Val. Loss: 1.346 |  Val. PPL:   3.843\n",
            "Epoch: 53 | Time: 0m 4s\n",
            "\tTrain Loss: 0.883 | Train PPL:   2.418\n",
            "\t Val. Loss: 1.325 |  Val. PPL:   3.761\n",
            "Epoch: 54 | Time: 0m 4s\n",
            "\tTrain Loss: 0.863 | Train PPL:   2.369\n",
            "\t Val. Loss: 1.303 |  Val. PPL:   3.679\n",
            "Epoch: 55 | Time: 0m 4s\n",
            "\tTrain Loss: 0.831 | Train PPL:   2.296\n",
            "\t Val. Loss: 1.288 |  Val. PPL:   3.627\n",
            "Epoch: 56 | Time: 0m 4s\n",
            "\tTrain Loss: 0.802 | Train PPL:   2.230\n",
            "\t Val. Loss: 1.282 |  Val. PPL:   3.603\n",
            "Epoch: 57 | Time: 0m 4s\n",
            "\tTrain Loss: 0.784 | Train PPL:   2.191\n",
            "\t Val. Loss: 1.260 |  Val. PPL:   3.527\n",
            "Epoch: 58 | Time: 0m 4s\n",
            "\tTrain Loss: 0.762 | Train PPL:   2.143\n",
            "\t Val. Loss: 1.240 |  Val. PPL:   3.457\n",
            "Epoch: 59 | Time: 0m 4s\n",
            "\tTrain Loss: 0.742 | Train PPL:   2.101\n",
            "\t Val. Loss: 1.226 |  Val. PPL:   3.408\n",
            "Epoch: 60 | Time: 0m 4s\n",
            "\tTrain Loss: 0.716 | Train PPL:   2.045\n",
            "\t Val. Loss: 1.217 |  Val. PPL:   3.376\n",
            "Epoch: 61 | Time: 0m 4s\n",
            "\tTrain Loss: 0.698 | Train PPL:   2.010\n",
            "\t Val. Loss: 1.206 |  Val. PPL:   3.340\n",
            "Epoch: 62 | Time: 0m 4s\n",
            "\tTrain Loss: 0.673 | Train PPL:   1.961\n",
            "\t Val. Loss: 1.192 |  Val. PPL:   3.295\n",
            "Epoch: 63 | Time: 0m 4s\n",
            "\tTrain Loss: 0.656 | Train PPL:   1.927\n",
            "\t Val. Loss: 1.179 |  Val. PPL:   3.250\n",
            "Epoch: 64 | Time: 0m 4s\n",
            "\tTrain Loss: 0.640 | Train PPL:   1.896\n",
            "\t Val. Loss: 1.165 |  Val. PPL:   3.206\n",
            "Epoch: 65 | Time: 0m 4s\n",
            "\tTrain Loss: 0.616 | Train PPL:   1.852\n",
            "\t Val. Loss: 1.158 |  Val. PPL:   3.182\n",
            "Epoch: 66 | Time: 0m 4s\n",
            "\tTrain Loss: 0.595 | Train PPL:   1.813\n",
            "\t Val. Loss: 1.139 |  Val. PPL:   3.124\n",
            "Epoch: 67 | Time: 0m 4s\n",
            "\tTrain Loss: 0.583 | Train PPL:   1.791\n",
            "\t Val. Loss: 1.134 |  Val. PPL:   3.107\n",
            "Epoch: 68 | Time: 0m 4s\n",
            "\tTrain Loss: 0.559 | Train PPL:   1.749\n",
            "\t Val. Loss: 1.130 |  Val. PPL:   3.096\n",
            "Epoch: 69 | Time: 0m 4s\n",
            "\tTrain Loss: 0.546 | Train PPL:   1.726\n",
            "\t Val. Loss: 1.125 |  Val. PPL:   3.081\n",
            "Epoch: 70 | Time: 0m 4s\n",
            "\tTrain Loss: 0.530 | Train PPL:   1.698\n",
            "\t Val. Loss: 1.107 |  Val. PPL:   3.026\n",
            "Epoch: 71 | Time: 0m 4s\n",
            "\tTrain Loss: 0.513 | Train PPL:   1.670\n",
            "\t Val. Loss: 1.091 |  Val. PPL:   2.978\n",
            "Epoch: 72 | Time: 0m 4s\n",
            "\tTrain Loss: 0.503 | Train PPL:   1.654\n",
            "\t Val. Loss: 1.089 |  Val. PPL:   2.972\n",
            "Epoch: 73 | Time: 0m 4s\n",
            "\tTrain Loss: 0.488 | Train PPL:   1.629\n",
            "\t Val. Loss: 1.084 |  Val. PPL:   2.958\n",
            "Epoch: 74 | Time: 0m 4s\n",
            "\tTrain Loss: 0.472 | Train PPL:   1.603\n",
            "\t Val. Loss: 1.071 |  Val. PPL:   2.919\n",
            "Epoch: 75 | Time: 0m 4s\n",
            "\tTrain Loss: 0.458 | Train PPL:   1.581\n",
            "\t Val. Loss: 1.056 |  Val. PPL:   2.874\n",
            "Epoch: 76 | Time: 0m 4s\n",
            "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
            "\t Val. Loss: 1.067 |  Val. PPL:   2.905\n",
            "Epoch: 77 | Time: 0m 4s\n",
            "\tTrain Loss: 0.428 | Train PPL:   1.535\n",
            "\t Val. Loss: 1.056 |  Val. PPL:   2.875\n",
            "Epoch: 78 | Time: 0m 4s\n",
            "\tTrain Loss: 0.419 | Train PPL:   1.521\n",
            "\t Val. Loss: 1.037 |  Val. PPL:   2.821\n",
            "Epoch: 79 | Time: 0m 4s\n",
            "\tTrain Loss: 0.406 | Train PPL:   1.501\n",
            "\t Val. Loss: 1.049 |  Val. PPL:   2.856\n",
            "Epoch: 80 | Time: 0m 4s\n",
            "\tTrain Loss: 0.392 | Train PPL:   1.479\n",
            "\t Val. Loss: 1.044 |  Val. PPL:   2.839\n",
            "Epoch: 81 | Time: 0m 4s\n",
            "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
            "\t Val. Loss: 1.048 |  Val. PPL:   2.853\n",
            "Epoch: 82 | Time: 0m 4s\n",
            "\tTrain Loss: 0.368 | Train PPL:   1.444\n",
            "\t Val. Loss: 1.024 |  Val. PPL:   2.784\n",
            "Epoch: 83 | Time: 0m 4s\n",
            "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
            "\t Val. Loss: 1.031 |  Val. PPL:   2.805\n",
            "Epoch: 84 | Time: 0m 4s\n",
            "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
            "\t Val. Loss: 1.006 |  Val. PPL:   2.734\n",
            "Epoch: 85 | Time: 0m 4s\n",
            "\tTrain Loss: 0.340 | Train PPL:   1.404\n",
            "\t Val. Loss: 1.023 |  Val. PPL:   2.783\n",
            "Epoch: 86 | Time: 0m 4s\n",
            "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
            "\t Val. Loss: 1.028 |  Val. PPL:   2.795\n",
            "Epoch: 87 | Time: 0m 4s\n",
            "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
            "\t Val. Loss: 1.028 |  Val. PPL:   2.795\n",
            "Epoch: 88 | Time: 0m 4s\n",
            "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
            "\t Val. Loss: 1.017 |  Val. PPL:   2.765\n",
            "Epoch: 89 | Time: 0m 4s\n",
            "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
            "\t Val. Loss: 1.006 |  Val. PPL:   2.735\n",
            "Epoch: 90 | Time: 0m 4s\n",
            "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
            "\t Val. Loss: 1.023 |  Val. PPL:   2.781\n",
            "Epoch: 91 | Time: 0m 4s\n",
            "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
            "\t Val. Loss: 1.011 |  Val. PPL:   2.748\n",
            "Epoch: 92 | Time: 0m 4s\n",
            "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
            "\t Val. Loss: 1.004 |  Val. PPL:   2.728\n",
            "Epoch: 93 | Time: 0m 4s\n",
            "\tTrain Loss: 0.276 | Train PPL:   1.317\n",
            "\t Val. Loss: 1.012 |  Val. PPL:   2.751\n",
            "Epoch: 94 | Time: 0m 4s\n",
            "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
            "\t Val. Loss: 1.014 |  Val. PPL:   2.758\n",
            "Epoch: 95 | Time: 0m 4s\n",
            "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
            "\t Val. Loss: 1.017 |  Val. PPL:   2.764\n",
            "Epoch: 96 | Time: 0m 4s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
            "\t Val. Loss: 1.023 |  Val. PPL:   2.781\n",
            "Epoch: 97 | Time: 0m 4s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
            "\t Val. Loss: 1.009 |  Val. PPL:   2.743\n",
            "Epoch: 98 | Time: 0m 4s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
            "\t Val. Loss: 1.024 |  Val. PPL:   2.784\n",
            "Epoch: 99 | Time: 0m 4s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
            "\t Val. Loss: 1.000 |  Val. PPL:   2.718\n",
            "Epoch: 100 | Time: 0m 4s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.988 |  Val. PPL:   2.686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbCGyO4ZEzFm"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztR5mNm8EzFn"
      },
      "source": [
        "Finally, we test the model on the test set using these \"best\" parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5F5wPqT60FC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e487958-3f71-4a20-fb23-7774cddddfe0"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/My Drive/END/session-15/english-python.pt'))\n",
        "\n",
        "valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "print(f'| Validation Loss: {valid_loss:.3f} ')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Validation Loss: 0.988 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5TItCLNV57f"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "    \n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otFG6WuZWBTN"
      },
      "source": [
        "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
        "    \n",
        "    assert n_rows * n_cols == n_heads\n",
        "    \n",
        "    fig = plt.figure(figsize=(15,25))\n",
        "    \n",
        "    for i in range(n_heads):\n",
        "        \n",
        "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
        "        \n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                           rotation=45)\n",
        "        ax.set_yticklabels(['']+translation)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr3O_4Fs9Xvq",
        "outputId": "29b2db66-899c-4af0-fcc0-7b7802d02467"
      },
      "source": [
        "for i in range(20):\n",
        "  src = vars(valid_dataset.examples[i])['English']\n",
        "  trg = vars(valid_dataset.examples[i])['Python']\n",
        "  translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "  translation_sent = \" \".join(translation)\n",
        "  trg_sent = \" \".join(trg)\n",
        "  src_sent = \" \".join(src)\n",
        "  translation_sent = translation_sent.replace(\"NEWLINE\", \"\\n\")\n",
        "  translation_sent = translation_sent.replace(\"INDENT\", \"\\t\")\n",
        "  translation_sent = translation_sent.replace(\"DEDENT\", \"\")\n",
        "\n",
        "  print(\"Example: \", i)\n",
        "  print(src_sent)\n",
        "  print(\"__________________\")\n",
        "  print(translation_sent)\n",
        "  print(\"\\n\")\n",
        "\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example:  0\n",
            "write a python program to break when the num is perfectly divisible\n",
            "__________________\n",
            "def comp_power ( lst ) : \n",
            " \t return list ( map ( lambda x : x ** 1.5 , lst ) ) \n",
            "  <eos>\n",
            "\n",
            "\n",
            "Example:  1\n",
            "please write a program which count and print the numbers of each character in a string input by console\n",
            "__________________\n",
            "dic = { } \n",
            " s = \"JRR Tolkien\" \n",
            " for s in s : \n",
            " \t dic [ s ] = dic . get ( s , 0 ) + 1 \n",
            "  print ( '\\n' . join ( [ '%s,%s' % ( k , v ) for k ,\n",
            "\n",
            "\n",
            "Example:  2\n",
            "generating a reliable secure random number\n",
            "__________________\n",
            "import secrets \n",
            " print ( \"Random integer number generated using secrets module is \" ) \n",
            " number = secrets . randbelow ( 30 ) \n",
            " print ( number ) \n",
            " <eos>\n",
            "\n",
            "\n",
            "Example:  3\n",
            "write a python function to add elements of two lists\n",
            "__________________\n",
            "def add_two_lists ( list1 , list2 ) : \n",
            " \t list1 = [ 1 , 2 , 3 ] \n",
            " list2 = [ 4 , 5 , 6 ] \n",
            " sum_list = [ ] \n",
            " for ( item1 , item2 ) in zip ( list1 , list2 ) : \n",
            "\n",
            "\n",
            "\n",
            "Example:  4\n",
            "write a function which take unlimited number and add it\n",
            "__________________\n",
            "def str_uppercase ( s : str ) : \n",
            " \t return s . upper ( ) \n",
            "  <eos>\n",
            "\n",
            "\n",
            "Example:  5\n",
            "write a python function to reverse words in a string\n",
            "__________________\n",
            "def rev_sentence ( sentence ) : \n",
            " \t words = sentence . split ( ' ' ) \n",
            " reverse_sentence = ' ' . join ( reversed ( words ) ) \n",
            " return reverse_sentence \n",
            "  if __name__ == \"__main__\" : \n",
            " \t string [ \"ROHAN\" , \"END\" ] \n",
            " <eos>\n",
            "\n",
            "\n",
            "Example:  6\n",
            "32 write a python program to find hcf or gcd using euclidian algorithm\n",
            "__________________\n",
            "def compute_hcf ( x , y ) : \n",
            " \t while ( y ) : \n",
            " \t x , y = y , x % y \n",
            "  return x \n",
            "  <eos>\n",
            "\n",
            "\n",
            "Example:  7\n",
            "write a python function split a given file path into filename and parent directory\n",
            "__________________\n",
            "def inspect_func ( fn ) : \n",
            " \t from inspect import isfunction , ismethod \n",
            " print ( f'{fn} is method {ismethod(func)}' ) \n",
            " print ( f'{fn} is function {isfunction(func)}' ) \n",
            "  <eos>\n",
            "\n",
            "\n",
            "Example:  8\n",
            "write a python function that counts the number of blank spaces in a text file\n",
            "__________________\n",
            "def count_blank_space ( ) : \n",
            " \t fname = input ( \"file name:\" ) \n",
            " count = 0 \n",
            " with open ( fname , 'r' ) as f : \n",
            " \t for line in f : \n",
            " \t count += line . count ( ' ' ) \n",
            "   return count\n",
            "\n",
            "\n",
            "Example:  9\n",
            "write a python function to calculate the greatest common divisor gcd of two user provided positive integers\n",
            "__________________\n",
            "def lcm ( num1 , num2 ) : \n",
            " \t if num1 > num2 : \n",
            " \t z = num1 \n",
            "  else : \n",
            " \t z = num2 \n",
            "  while ( True ) : \n",
            " \t if ( ( ( z % num1 == 0 ) and (\n",
            "\n",
            "\n",
            "Example:  10\n",
            "write a python function to find the total sum of a nested list using recursion\n",
            "__________________\n",
            "from collections import Iterable \n",
            " def flatten ( lis ) : \n",
            " \t for item in lis : \n",
            " \t if isinstance ( item , Iterable ) and not isinstance ( item , str ) : \n",
            " \t for x in flatten ( item ) : \n",
            " \t yield x\n",
            "\n",
            "\n",
            "Example:  11\n",
            "write a function to calculate the simple interest for principal p rate r and time in years y\n",
            "__________________\n",
            "def get_ci ( p : float , r : float , t : float , n : float ) -> float : \n",
            " \t return round ( p * ( 1 + ( r / ( n * 100 ) + ( n * t ) ) ** ( n\n",
            "\n",
            "\n",
            "Example:  12\n",
            "write a python program to count the occurrences of a word in a text file\n",
            "__________________\n",
            "def isPalindrome ( s ) : \n",
            " \t return s == s [ : : - 1 ] \n",
            "  <eos>\n",
            "\n",
            "\n",
            "Example:  13\n",
            "write a function to right rotate a given list by given input\n",
            "__________________\n",
            "def rotate_left ( input , d ) : \n",
            " \t Lfirst = input [ 0 : d ] \n",
            " Lsecond = input [ d : ] \n",
            " return ( Lsecond + Lfirst ) \n",
            "  <eos>\n",
            "\n",
            "\n",
            "Example:  14\n",
            "write a function that returns a list sorted descending\n",
            "__________________\n",
            "def remove_duplicatesinlist ( lst ) : \n",
            " \t return len ( lst ) == len ( set ( lst ) ) \n",
            "  <eos>\n",
            "\n",
            "\n",
            "Example:  15\n",
            "write a python function to return the dot product of two vectors\n",
            "__________________\n",
            "def is_prod_even ( num1 , num2 ) : \n",
            " \t prod = num1 * num2 \n",
            " return not prod % 2 \n",
            "  <eos>\n",
            "\n",
            "\n",
            "Example:  16\n",
            "python program to remove punctuations from a string\n",
            "__________________\n",
            "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~''' \n",
            " my_str = \"Hello!!!, he said ---and went.\" \n",
            " no_punct = \"\" \n",
            " for char in my_str : \n",
            " \t if char not in punctuations : \n",
            " \t no_punct = no_punct + char \n",
            "   print ( no_punct ) \n",
            " <eos>\n",
            "\n",
            "\n",
            "Example:  17\n",
            "generating a reliable secure random number\n",
            "__________________\n",
            "import secrets \n",
            " print ( \"Random integer number generated using secrets module is \" ) \n",
            " number = secrets . randbelow ( 30 ) \n",
            " print ( number ) \n",
            " <eos>\n",
            "\n",
            "\n",
            "Example:  18\n",
            "write a python function that deletes the last element of a list and returns the list and the deleted element\n",
            "__________________\n",
            "def delete_last_element ( list_to_be_processed ) : \n",
            " \t deleted_element = list_to_be_processed . pop ( ) \n",
            " return list_to_be_processed , deleted_element \n",
            "  <eos>\n",
            "\n",
            "\n",
            "Example:  19\n",
            "write a program to check if a string is binary or not\n",
            "__________________\n",
            "str1 = \"01110011 a\" \n",
            " set1 = set ( str1 ) \n",
            " if set1 == { '0' , '1' } or set1 == { '0' } or set1 == { '1' } : \n",
            " \t print ( \"string is binary\" ) \n",
            "  else : \n",
            " \t print ( \"string is not binary\" ) \n",
            " \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGkRVPXrWVRv"
      },
      "source": [
        "example_idx = 8\n",
        "\n",
        "src = vars(train_dataset.examples[example_idx])['English']\n",
        "trg = vars(train_dataset.examples[example_idx])['Python']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkMKpWU3W7CQ"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0i_0KwbXWCz"
      },
      "source": [
        "example_idx = 0\n",
        "\n",
        "src = vars(train_dataset.examples[example_idx])['English']\n",
        "trg = vars(train_dataset.examples[example_idx])['Python']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqlNsBZ5XZv8"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY7SsC8TEzFn"
      },
      "source": [
        "Just looking at the test loss, we get better performance. This is a pretty good sign that this model architecture is doing something right! Relieving the information compression seems like the way forard, and in the next tutorial we'll expand on this even further with *attention*."
      ]
    }
  ]
}